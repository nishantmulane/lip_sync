# Wav2Lip - Lip-Sync Model

This repository implements the Wav2Lip model to generate realistic lip-sync videos from an image and audio file.

## ğŸ“ Project Structure

```
Wav2Lip/
â”œâ”€â”€ README.md                  # Project Documentation
â”œâ”€â”€ checkpoints/               # Model weights directory
â”‚    â”œâ”€â”€ README.md             # Model weights info
â”‚    â”œâ”€â”€ face_sync.pth         # SyncNet model weight (for lip-sync evaluation)
â”‚    â””â”€â”€ wav2lip_gan.pth       # Wav2Lip GAN model weight
â”œâ”€â”€ inputs/                    # Input media directory
â”‚    â”œâ”€â”€ image.jpg             # Input face image
â”‚    â””â”€â”€ input_audio.mp3       # Input audio file
â”œâ”€â”€ outputs/                   # Output media directory
â”‚    â””â”€â”€ result.mp4            # Lip-synced output video
â”œâ”€â”€ inference.py               # Main script for inference
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ ...                        # Other training and evaluation scripts
```

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/Wav2Lip.git
cd Wav2Lip
```

### 2. Create and Activate a Virtual Environment

```bash
python -m venv wav2lip_env
source wav2lip_env/bin/activate  # On macOS/Linux
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Download Pre-trained Models

Ensure the following weights exist under the `checkpoints/` folder:

- [wav2lip_gan.pth](https://github.com/Rudrabha/Wav2Lip/releases/download/v1.0/wav2lip_gan.pth)
- [face_sync.pth](https://github.com/Rudrabha/Wav2Lip/releases/download/v1.0/face_sync.pth)

Download and place them in `checkpoints/` folder:

```bash
wget "https://github.com/Rudrabha/Wav2Lip/releases/download/v1.0/wav2lip_gan.pth" -P checkpoints/
wget "https://github.com/Rudrabha/Wav2Lip/releases/download/v1.0/face_sync.pth" -P checkpoints/
```

## â–¶ï¸ Running the Model

Ensure you have an image and audio file in the `inputs/` folder.

```bash
python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth \
    --face inputs/image.jpg --audio inputs/input_audio.mp3 \
    --outfile outputs/result.mp4
```

### Example Input Files:
- `inputs/image.jpg`: Image to animate.
- `inputs/input_audio.mp3`: Audio for lip-sync.

### Output:
- `outputs/result.mp4`: Generated lip-synced video.f

## ğŸ“š References

- [Wav2Lip Paper](https://arxiv.org/abs/2008.10010)
- [Original Wav2Lip Repository](https://github.com/Rudrabha/Wav2Lip)


